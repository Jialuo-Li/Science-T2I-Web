<!doctype html>
<html lang="en">
    <head>
        <style>
            pre, code {
                font-size: 16px;
            }
        </style>

        <title>Science-T2I: Addressing Scientific Illusions in Image Synthesis</title>
        <link rel="icon" type="image/x-icon" href="static/img/icon/Science-T2I.png">

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Open Graph -->
        <meta property="og:url" content="" />
        <meta property="og:image" content="./static/img/preview.png" />
        <meta property="og:title" content="Science-T2I: Addressing Scientific Illusions in Image Synthesis" />
        
        <!-- Twitter -->
        <meta name="twitter:url" content="https://jialuo-li.github.io/Science-T2I-page/" />
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:image" content="./static/img/preview.png" />
        <meta name="twitter:title" content="Science-T2I: Addressing Scientific Illusions in Image Synthesis" />
        <meta name="twitter:description" content="We introduce VSI-Bench, a novel benchmark of over 5,000 video-based visual-spatial intelligence questions, to evaluate and probe MLLMs, which revealed that their emerging spatial reasoning and local world modeling capabilities remain subhuman but promising." />

        <!-- JS Libraries -->
        <script src="./static/js/distill_template.v2.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script defer="" src="./static/js/hider.js"></script>
        <script defer src="./static/js/image_interact.js"></script>
        <script defer src="./static/js/switch_videos.js"></script>
        <script defer src="./static/js/video-speed.js"></script>

        <!-- Stylesheets -->
        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        
        <!-- KaTeX -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

        <!-- FontAwesome -->
        <script defer src="./static/js/fontawesome.all.min.js"></script>

        <!-- Medium Zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>

        <style>
            figure {
                width: 150%;
                margin-left: -20%;
            }
        </style>
    </head>
    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px"><i>Science-T2I</i></h1>
                    <div class="responsive-header">
                        <h2>Addressing Scientific Illusions in Image Synthesis</h2>
                    </div>

                    <p style="color: #3361aa">
                        We introduce the PISA (<em><strong style="color: #4a14f1">P</strong></em>hysics-<em><strong style="color: #4a14f1">I</strong></em>nformed <em><strong style="color: #4a14f1">S</strong></em>imulation and <em><strong style="color: #4a14f1">A</strong></em>lignment) framework for studying physics post-training.
                    </p>
                    <div class="icon-container">
                        <div class="icon-item">
                            <img src="./static/img/icons/benchmark.svg" alt="PISA Bench Icon">
                            <div><strong>PisaBench</strong>: We introduce PisaBench to examine the ability of video generative models to produce accurate physical phenomena by focusing on a straightforward dropping task.</div>
                        </div>
                        <div class="icon-item">
                            <img src="./static/img/icons/enhancement.svg" alt="Physics Post-Training Icon">
                            <div><strong>Physics Post-Training</strong>: We present a two-stage post-training pipeline to enhance the physical accuracy of video diffusion models.</div>
                        </div>
                        <div class="icon-item">
                            <img src="./static/img/icons/generalization.svg" alt="Generalization Analysis Icon">
                            <div><strong>Generalization Analysis</strong>: We conduct a series of experiments to examine the our model's learned behavior and generalization ability.</div>
                        </div>
                    </div>
                    <div class="button-container">
                        <a href="https://arxiv.org/pdf/2503.09595" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="ai ai-arxiv"></i>
                            </span>
                            arXiv
                        </a>
                        <a href="./static/PISA.pdf" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="fas fa-file-pdf"></i>
                            </span>
                            <span>pdf</span>
                        </a>
                        <a href="https://github.com/Jialuo-Li/Science-T2I" class="button" target="_blank">
                            <span class="icon is-small">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                        </a>
                        <a href="https://huggingface.co/collections/Jialuo21/science-t2i-67d3bfe43253da2bc7cfaf06" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>Pisa</span>
                        </a>
                    </div>
                </div>
                <div class="header-image">
                    <img src="static/img/teaser_img.png" alt="Teaser Image" class="teaser-image">
                </div>
            </div>
        </div>
    <d-article>
        <div class="byline">
            <div class="byline-container">
                <p>
                    <a href="https://jialuo-li.github.io/" class="author-link" target="_blank" style="margin-right: 15px;">Jialuo Li<sup>△</sup></a>
                    <a href="https://rese1f.github.io/" class="author-link" target="_blank" style="margin-right: 15px;">Wenhao Chai<sup>▲</sup></a>
                    <a href="https://zeyofu.github.io/" class="author-link" target="_blank">Xingyu Fu<sup>◆</sup></a>
                </p>
                <p>
                    <a href="https://xxuhaiyang.github.io/" class="author-link" target="_blank" style="margin-right: 15px;">Haiyang Xu<sup>◇</sup></a>
                    <a href="https://www.sainingxie.com/" class="author-link" target="_blank">Saining Xie<sup>△</sup></a>
                </p>
                <p></p>
                <p></p>
                <p style="text-align: center;">
                    <a class="affiliation-link" target="_blank" style="margin-right: 15px;"><sup>△</sup>New York University</a>
                    <a class="affiliation-link" target="_blank"><sup>▲</sup>University of Washington</a>
                </p>
                <p style="text-align: center;">
                    <a class="affiliation-link" target="_blank" style="margin-right: 15px;"><sup>◆</sup>University of Pennsylvania</a>
                    <a class="affiliation-link" target="_blank"><sup>◇</sup>University of California, San Diego</a>
                </p>

            </div>
        </div>
        
        <d-figure id="fig-teaser">
            <figure>
                <img data-zoomable="" draggable="false" src="static/img/teaser.png" alt="Science-T2I">
                <figcaption>
                    <strong>Fig 1:</strong> When presented with knowledge-implicit prompts, can LMMs and VLMs effectively distinguish between real and fake scientific images? Can generative models produce scientifically plausible images from such prompts? Does fine-tuning generative models with relevant data enhance their ability to generalize based on knowledge? To explore these questions, we establish a benchmark for evaluating LMMs and VLMs, construct a dataset to train a reward model which can then serve as a reliable tool for assessing generative models, and fine-tune generative models to investigate the generalization.
                </figcaption>
            </figure>
        </d-figure>

        <p class="text abstract">
            We present a novel approach to integrating scientific knowledge into generative models, enhancing their realism and consistency in image synthesis. First, we introduce <b>Science-T2I</b>, an expert-annotated adversarial dataset comprising adversarial 20k image pairs with 9k prompts, covering wide distinct scientific knowledge categories. Leveraging <b>Science-T2I</b>, We present <b>SciScore</b>, an end-to-end reward model that refines the assessment of generated images based on scientific knowledge, which is achieved by augmenting both the scientific comprehension and visual capabilities of pre-trained CLIP model. Additionally, based on <b>Science-T2I</b>, we propose a two-stage training framework, comprising a supervised fine-tuning phase and a masked online fine-tuning phase, to incorporate scientific knowledge into existing generative models. Through comprehensive experiments, we demonstrate the effectiveness of our framework in establishing new standards for evaluating the scientific realism of generated content. Specifically, <b>SciScore</b> attains performance comparable to human-level, demonstrating a 5% improvement similar to evaluations conducted by experienced human evaluators. Furthermore, by applying our proposed fine-tuning method to FLUX, we achieve a performance enhancement exceeding 50% based on <b>SciScore</b>.
        </p>
        <div class="icon-row">
            <a href="#pisabench" class="icon-link">
                <img src="static/img/icons/benchmark.svg" alt="Pisa Bench Logo" class="icon-jump">
                PisaBench<br> &nbsp;
            </a>
            <a href="#post-training" class="icon-link">
                <img src="static/img/icons/enhancement.svg" alt="Physics Post-Training Logo" class="icon-jump">
                Physics<br>Post-Training
            </a>
            <a href="#generalization" class="icon-link">
                <img src="static/img/icons/generalization.svg" alt="Generalization Analysis Logo" class="icon-jump">
                Generalization<br>Analysis
            </a>
        </div>
        <p class="click-hint2" style="width: 85%;">
            <img src="static/img/icon/click.gif" style="width: 1.5rem">
            <strong>Click to jump to each section.</strong>
        </p>
        <hr>

        <div id="science-t2i">
            <h1 class="text">Science-T2I: An Adversarial Dataset Spanning Scientific Disciplines</h1>
            <p class="text"><strong>Task overview.</strong> Science-T2I consists of 16 tasks spanning physics, chemistry and biology that require the model to infer or visualize concepts not explicitly stated in the prompts but rooted in underlying scientific principles.
            </p>
            
            <d-figure id="fig-data-stat">
              <figure>
                  <img data-zoomable="" draggable="false" src="static/img/data_stat.png">
                  <figcaption style="text-align: center; margin: 20px;">
                    <strong>Fig 2:</strong> Data statistics (left) of and wordcloud (right) of Science-T2I.
                  </figcaption>
              </figure>
            </d-figure>
  
            <p class="text">
                <strong>Task classification.</strong> Beyond a classification based on scientific disciplines, the tasks can be categorized into two distinct groups:
                <ol>
                    <li style="margin-bottom: 3px;"><strong>Subject-oriented task (ST)</strong>
                        require scientific reasoning to discern how inherent differences between subjects lead to varying visual features under identical conditions.
                    </li>
                    <li style="margin-bottom: 3px;"><strong>Condition-oriented task (CT)</strong>
                        focus on how a single condition affects various subjects. Scientific reasoning in these tasks centers on the applied condition, not the subject's individual properties. 
                    </li>
                </ol>
            </p>
            
            <d-figure id="fig-classification">
              <figure>
                  <img data-zoomable="" draggable="false" src="static/img/classification.png">
                  <figcaption style="text-align: center; margin: 20px;">
                    <strong>Fig 3:</strong> Task classification of Science-T2I.
                  </figcaption>
              </figure>
            </d-figure>
  
            <p class="text"><strong>Prompt design.</strong> In Science-T2I, we categorize prompts into three types based on their use in scientific reasoning:
            </p>
            <ol style="margin-left: 10px;">
              <li style="margin-bottom: 3px;"><strong>Implicit prompt (IP).</strong>
                Contains terms implying visual characteristics or phenomena requiring scientific interpretation (e.g., "an unripe apple" suggesting greenness).
              </li>
              <li style="margin-bottom: 3px;"><strong>Explicit prompt (EP).</strong>
                Reformulates the IP into a clear, descriptive statement that results in a scientifically accurate depiction (e.g., "a green apple" explicitly conveying immaturity).
              </li>
              <li style="margin-bottom: 3px;"><strong>Superficial prompt (SP).</strong>
                Provides explicit but scientifically inaccurate descriptions, focusing on surface-level interpretations (e.g., interpreting "an unripe apple" as "a red apple").
              </li>
            </ol>
  
            <d-figure id="fig-data-pipeline">
              <figure>
                  <img data-zoomable="" draggable="false" src="static/img/dataset_pipeline.png">
                  <figcaption style="text-align: center; margin: 20px;">
                    <strong>Fig 4:</strong> Data curation pipeline of Science-T2I.
                  </figcaption>
              </figure>
            </d-figure>
  
            <p class="text"><strong>Data curation.</strong> We utilize GPT-4o to create templates and generate corresponding prompts during the data curation process. These outputs are then used to guide T2I models for image generation. Following this, human annotators review and filter the data, incorporating insights from an additional website knowledgebase to ensure the reliability and accuracy of the final dataset.
            </p>
            
            <p class="text">
                <p class="text">
                    As shown in <a href="#tab:evaluation_results">Table 1</a>, the results of running the baseline models on the benchmark indicate a consistent failure to generate physically accurate dropping behavior, despite the visual realism of their generated frames.
                    Qualitatively, we see common failure cases in the above showcases, such as implausible object deformations, floating, hallucination of new objects, and unrealistic special effects. We further visualize a random subset of generated trajectories on the left of <a href="#fig:trajectory">Figure 3</a>. In many cases, the object remains completely static, and sometimes the object even moves upward. When downward motion is present, it is often slow or contains unrealistic horizontal movement.
                </p>
            </p>
            <d-figure id="fig:trajectory">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/trajectory.png" alt="trajectory visulization">
                    <figcaption>
                        <strong>Figure 3: Trajectory visualization. </strong>On the left, we plot random trajectories from the baseline models in Table 1. On the right, we show random trajectories from our fine-tuned model. The baseline trajectories exhibit unrealistic behavior, and most of them stay completely static. On the right, we see the trajectories consistently falling downward with collision and rolling behavior being modeled after the point of contact.
                    </figcaption>
                </figure>
            </d-figure>
        </div>
        
        <div id="post-training">
            <h1 class="text">Physics Post-Training</h1>
            <p class="text">
                <p class="text">
                    We present a post-training process to address the limitations of current models. We utilize simulated videos that demonstrate realistic dropping behavior. Our approach for post-training is inspired by the two-stage pipeline consisting of supervised fine-tuning followed by reward modeling commonly used in LLMs.
                </p>
            </p>
            <p class="text">
                <p class="text">
                    <strong>Simulated Adaptation Data. </strong>
                    We use Kubric to create simulated videos of objects dropping and colliding with other objects on the ground. Each video consists of 1-6 dropping objects onto a (possibly empty) pile of up to 4 objects underneath them. The videos are 2 seconds long, consisting of 32 frames at 16 fps. The objects are sourced from the Google Scanned Objects (GSO) dataset, which provides true-to-scale 3D models created from real-world scans across diverse categories.
                </p>
            </p>
            <p class="text">
                <p class="text">
                    <strong>Physics Supervised Fine-Tuning (PSFT). </strong>
                    In the first stage, we use the pretrained Open-Sora v1.2 as our base model and fine-tune it on our simulated video dataset. We employ Open-Sora v1.2's rectified flow training objective without modification.
                    We have the following findings:
                    <ol class="text">
                        <li>Fine-tuning with this data alone is sufficient to induce realistic dropping behavior in the model. (See <a href="#tab:evaluation_results">Table 1</a>)</li>
                        <li>Only 5,000 samples are needed to achieve optimal results. (See <a href="fig:training_curve">Figure 4</a>)</li>
                        <li>The learned knowledge from Open-Sora's pretraining plays a critical role in our task. (See <a href="fig:training_curve">Figure 4</a>)</li>
                    </ol>
                </p>
                <d-figure id="fig:training_curve">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/training_curve.png" alt="training curve">
                        <figcaption>
                            <strong>Figure 4. </strong>Plots (a), (b), and (c) demonstrate that our metrics tend to improve with further training and that leveraging a pre-trained video diffusion model enhances performance compared to random initialization. In plot (d), the size of the training dataset varies in each training run (each consisting of 5k steps). With only 5k samples, we can achieve optimal results.
                        </figcaption>
                    </figure>
                </d-figure>
            </p>
            <p class="text">
                <p class="text">
                    <strong>Object Reward Optimization (ORO). </strong>
                    In the second stage, we propose an approach Object Reward Optimization (ORO) to use reward gradients to guide the video generation model toward generating videos where the object's motion and shape more closely align with the ground truth. We use a pretrained vision model (depth, optical flow, and segmentation) to compare the signals derived from the model's generation with the ground truth. This reward can then be backprobagated to the model through its denoising steps. Our approach is similar to the <a href="https://vader-vid.github.io/" target="_blank">VADER</a> framework, please see our paper for more details.
                </p>
            </p>
            <p class="text">
                <p class="text">
                We utilize SAM 2, RAFT, and Depth-Anything-V2 to generate segmenation masks, optical flow, and depth maps of the falling objects and define <strong>Segmentation Reward</strong>, <strong>Optical Flow Reward</strong>, and <strong>Depth Reward</strong> as follows:
                \[
                \begin{equation}
                R_{\text{seg}}(x_0', x_0) = \operatorname{IoU}(M^{\text{gen}}, M^{\text{gt}})\\
                R_{\text{flow}}(x_0', x_0) = -|V^{\text{gen}} - V^{\text{gt}}| \\
                R_{\text{depth}}(x_0', x_0) = -|D^{\text{gen}} - D^{\text{gt}}|
                \end{equation}
                \]
                </p>
            </p>
            <p class="text">
                <p class="text">
                We begin from the checkpoint of the first stage, which is trained on 5,000 samples trained over 5,000 gradient steps. We then fine-tune the model with ORO on the smiulated dataset. We have the following findings:
                <ol class="text">
                    <li>Incorporating ORO in reward modeling further improves performance. (See <a href="#tab:evaluation_results">Table 1</a>)</li>
                    <li>Each reward function enhances the aspect of physicality that aligns with its intended purpose—segmentation rewards improve shape accuracy, while flow rewards and depth rewards improve motion accuracy. (See <a href="#tab:evaluation_results">Table 1</a>)</li>
                </ol>
            </p>
        </div>
        <div id="generalization">
            <h1 class="text">Generalization Analysis</h1>
            <p class="text">
                <p class="text">
                Having introduced our post-training methodology, we probe further into the model's understanding of the interaction between gravity and perspective, the two laws that determine the dynamics of our videos. We first test if the learned physical behavior of our model can generalize to dropping heights and depths beyond its training distribution. Then, we study the ability of the model to learn the probability distribution induced by the uncertainty of perspective.
                </p>
            </p> 
            <p class="text">
                <p class="text">
                    <strong>Generalization to Unseen Depths and Heights. </strong>
                    Depth and height are the main factors that affect the dynamics of a falling object in our videos. We can see this by combining the laws of gravity with perspective under our camera assumptions to model the object's image \(y\) coordinate as a function of time: 
                    \[
                    y(t) = \frac{f}{Z} (Y_0 - \frac{1}{2} g t^2)
                    \]
                </p>
            </p>
            <p class="text">
                <p class="text">
                    We create a simulated test set in which a single object is dropped from varying depths and heights, using objects and backgrounds unseen during training. We create an in-distribution (ID) and out-of-distribution (OOD) test set respectively.
                </p>
            </p> 
            <div id="tab:depth_height_tab" style="display: flex; flex-direction: column; align-items: center;">
                <div class="table-container">
                    <table class="data-table">
                        <thead>
                        <tr>
                            <th colspan="1" class="tb-hdr">Setting</th>
                            <th colspan="1" class="tb-hdr">L2 (\(\downarrow\))</th>
                            <th colspan="1" class="tb-hdr">Chamfer Distance (\(\downarrow\))</th>
                            <th colspan="1" class="tb-hdr">IoU (\(\uparrow\))</th>
                            <th colspan="1" class="tb-hdr">Time Error (\(\downarrow\))</th>
                        </tr>
                        </thead>
                        <tbody>
                        <tr>
                            <td class="section-border">ID</td>
                            <td class="highlight">0.036</td>
                            <td class="highlight">0.088</td>
                            <td class="highlight">0.155</td>
                            <td class="highlight">0.049</td>
                        </tr>
                        <tr>
                            <tr>
                                <td class="section-border">OOD</td>
                                <td>0.044</td>
                                <td>0.143</td>
                                <td>0.049</td>
                                <td>0.187</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <figcaption>
                    <strong>Table 2: Results of our metrics on in-distribution (ID) and out-of-distribution (OOD) depth-height combinations. </strong> Depth values range from 1-5m (ID range \([1,3]\)) and height values range from 0.5-2.5 (ID range \([0.5,1.5]\)).
                </figcaption>
            </div>
            <p class="text">
                <p class="text">
                    As shown in <a href="#tab:depth_height_tab">Table 2</a>, our analysis shows that performance degrades for out-of-distribution scenarios. Since depth and height are the main physical quantities that affect falling dynamics, this finding indicates that our model may struggle to learn a fully generalizable law that accounts for the interaction of perspective and gravity.
                </p>
            </p>
            <p class="text">
                <p class="text">
                    <strong>Distributional Analysis.</strong>
                    The evolution of a physical system is not uniquely determined by a single initial image, since the lossy uncertainty of perspective induces a distribution of possible outcomes as shown in <a href="fig:ambiguity">Figure 5</a>. An ideal video world model should (1) output videos that are faithful to the evolution of some plausible world state and (2) provide accurate coverage across the entire distribution of the world that is possible from its conditioning signal. In this section, we examine these two facets by studying \(p(t|y)\): the distribution of dropping times possible from an object at coordinate y in the image plane. To do this, we create a simulated dataset that has a much wider distribution \(p(t|y)\) than our PSFT dataset.
                </p>
            </p>
            <d-figure id="fig:ambiguity">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/ambiguity.png" alt="ambiguity figure">
                    <figcaption>
                        <strong>Figure 5. </strong>Demonstration of ambiguity in 2D perspective projections. Each of the three clouds appears the exact same in the camera's image. The right side shows how we perform a scale and translation augmentation to generate deliberately ambiguous data.
                    </figcaption>
                </figure>
            </d-figure>
            <p class="text">
                <p class="text">
                    <strong>Testing (1): 3D faithfulness of trajectories.</strong>
                    After training our model on this new dataset, we test whether its trajectories are consistent with a valid 3D world state. We first obtain an estimated dropping time from generated videos using SAM2 masks. Using knowledge of the camera position, focal length, sensor width, and \(y\), we can obtain an implied depth and height of the trajectory. We can then back-project the video trajectory to 3D and analyze whether they constitute physically accurate trajectories. As show in in <a href="#fig:testing1">Figure 6</a>, we find that our model's lifted trajectories consistently align with the 3D trajectory at the height and depth implied by its dropping time, giving evidence that the model's visual outputs are faithful to some plausible real-world state.
                </p>
            </p>
            <d-figure id="fig:testing1">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/testing1.png" alt="testing1 figure">
                    <figcaption>
                        <strong>Figure 6: Examples of model trajectories lifted to 3D. </strong>The blue line represents the height of the camera ray passing through the bottom of the dropping object as a function of depth. The set of possible dropping trajectories at a given depth are depicted in gray. The lifted trajectory of the model is depicted in green.
                    </figcaption>
                </figure>
            </d-figure>

            <p class="text">
                <p class="text">
                    <strong>Testing (2): distributional alignment.</strong>
                    Going beyond the level of individual trajectories, we study the model's learned conditional distribution \(p(t|y)\). We create 50 different initial images with differing values of y, generate 128 different videos from each, and estimate the dropping time in each video. Using the laws of gravity, the laws of perspective, and the assumption of uniform depth sampling in our dataset, we can analytically derive the probability \(p(t|y)\) as: 
                    \[
                    \begin{equation}
                    p(t | y) =
                    \begin{cases}
                        \frac{gt}{(Z_{\max} - Z_{\min})\beta}, & t_{\min} \leq t \leq t_{\max} \\
                        0, & \text{otherwise}
                    \end{cases}
                    \end{equation}
                    \]
                </p>
            </p>
            <p class="text">
                <p class="text">
                    We then measure goodness-of-fit for each of the 50 experiments using the Kolmogorov-Smirnov (KS). The null hypothesis of our KS test is that the two distributions being compared are equal, and we consider p-values less than 0.05 as evidence of misalignment. Since our measured times have limited precision and can only take 32 distinct values—due to estimating the contact frame—we approximate the ground truth \(p(t|y)\) using a Monte Carlo method. We sample 1000 values from the ground truth distribution and then quantized them into 32 bins corresponding to their frame, which we use as ground truth observations in the KS test. We find that in all 50/50 cases, the p-value from the test is less than 0.05, which provides evidence that the model does not learn the correct distribution of dropping times. We visualize the misalignment between the empirical cdf of the model's in <a href="fig:testing2">Figure 7</a>.
                </p>
            </p>
            <d-figure id="fig:testing1">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/testing2.png" alt="testing1 figure">
                    <figcaption>
                        <strong>Figure 7: Visualizing \(p(t|y)\) misalignment for different images.</strong>Green shows the ground-truth CDF, orange is the 32-frame quantized version, and blue is the empirical CDF of 128 different samples of dropping times from the model.
                    </figcaption>
                </figure>
            </d-figure>
        </div>
        <div id="Conclusion">
            <h1 class="text">Conclusion</h1>
            <p class="text">
                <p class="text">
                    This work studies post-training as an avenue for adapting adapting pre-trained video generator into world models. We introduce a post-training strategy that is highly effective in aligning our model. Our work raises interesting insights into the learned distributions of generative models. Qualitatively, large scale image or video generative models appear to excel at generating likely samples from the data distribution, but this alone does not imply that they match the data distribution well in its entirety. As long as a model is able to generate likely samples, global distributional misalignment is not necessarily a problem for content creation. However, this problem becomes critical for world models, where alignment across the entire distribution is necessary for faithful world simulation. The insights revealed by our study, made possible by our constrained and tractable setting, indicate that although post-training improves per-sample accuracy, general distributional alignment remains unsolved.
                </p>
            </p>
        </div>

        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{li2025pisa,<br>
                &nbsp;&nbsp;title={PISA Experiments: Exploring Physics Post-Training for Video Diffusion Models by Watching Stuff Drop},<br>
                &nbsp;&nbsp;author={Li, Chenyu and Michel, Oscar and Pan, Xichen and Liu, Sainan and Roberts, Mike and Xie, Saining},<br>
                &nbsp;&nbsp;journal={arXiv preprint arXiv:2503.09595},<br>
                &nbsp;&nbsp;year={2025}<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>
        
        <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>
        <script src="./static/js/nav-bar.js"></script>
        
        <script type="text/javascript">
        </script>
    </body>
</html>
