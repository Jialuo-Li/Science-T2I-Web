<!doctype html>
<html lang="en">
    <head>
        <style>
            pre, code {
                font-size: 16px;
            }
        </style>

        <title>Science-T2I: Addressing Scientific Illusions in Image Synthesis</title>
        <link rel="icon" type="image/x-icon" href="static/img/icon/Science-T2I.png">

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Open Graph -->
        <meta property="og:url" content="" />
        <meta property="og:image" content="./static/img/preview.png" />
        <meta property="og:title" content="Science-T2I: Addressing Scientific Illusions in Image Synthesis" />
        
        <!-- Twitter -->
        <meta name="twitter:url" content="https://jialuo-li.github.io/Science-T2I-page/" />
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:image" content="./static/img/preview.png" />
        <meta name="twitter:title" content="Science-T2I: Addressing Scientific Illusions in Image Synthesis" />
        <meta name="twitter:description" content="We introduce VSI-Bench, a novel benchmark of over 5,000 video-based visual-spatial intelligence questions, to evaluate and probe MLLMs, which revealed that their emerging spatial reasoning and local world modeling capabilities remain subhuman but promising." />

        <!-- JS Libraries -->
        <script src="./static/js/distill_template.v2.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script defer="" src="./static/js/hider.js"></script>
        <script defer src="./static/js/image_interact.js"></script>
        <script src="./static/js/bulma-carousel.min.js"></script>
        <script src="./static/js/bulma-slider.min.js"></script>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="./static/js/explorer-index.js"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
            integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
            crossorigin="anonymous"></script>
        <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script>


        <!-- Stylesheets -->
        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        <link rel="stylesheet" href="./static/css/bulma.min.css">
        <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
        <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
                integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
        <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bootstrap4.min.css" rel="stylesheet">
                
        <!-- KaTeX -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

        <!-- FontAwesome -->
        <script defer src="./static/js/fontawesome.all.min.js"></script>

        <!-- Medium Zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>

        <style>
            figure {
                width: 150%;
                margin-left: -20%;
            }
            .collapse-content {
                display: none;
                margin-top: 10px;
            }
            .collapse-content.is-active {
                display: block;
            }
        </style>
    </head>
    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px"><i>Science-T2I</i></h1>
                    <div class="responsive-header">
                        <h2>Addressing Scientific Illusions in Image Synthesis</h2>
                    </div>

                    <p style="color: #3361aa">
                        We introduce the PISA (<em><strong style="color: #4a14f1">P</strong></em>hysics-<em><strong style="color: #4a14f1">I</strong></em>nformed <em><strong style="color: #4a14f1">S</strong></em>imulation and <em><strong style="color: #4a14f1">A</strong></em>lignment) framework for studying physics post-training.
                    </p>
                    <div class="icon-container">
                        <div class="icon-item">
                            <img src="./static/img/icons/benchmark.svg" alt="PISA Bench Icon">
                            <div><strong>PisaBench</strong>: We introduce PisaBench to examine the ability of video generative models to produce accurate physical phenomena by focusing on a straightforward dropping task.</div>
                        </div>
                        <div class="icon-item">
                            <img src="./static/img/icons/enhancement.svg" alt="Physics Post-Training Icon">
                            <div><strong>Physics Post-Training</strong>: We present a two-stage post-training pipeline to enhance the physical accuracy of video diffusion models.</div>
                        </div>
                        <div class="icon-item">
                            <img src="./static/img/icons/generalization.svg" alt="Generalization Analysis Icon">
                            <div><strong>Generalization Analysis</strong>: We conduct a series of experiments to examine the our model's learned behavior and generalization ability.</div>
                        </div>
                    </div>
                    <div class="button-container">
                        <a href="https://arxiv.org/pdf/2503.09595" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="ai ai-arxiv"></i>
                            </span>
                            arXiv
                        </a>
                        <a href="./static/PISA.pdf" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="fas fa-file-pdf"></i>
                            </span>
                            <span>pdf</span>
                        </a>
                        <a href="https://github.com/Jialuo-Li/Science-T2I" class="button" target="_blank">
                            <span class="icon is-small">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                        </a>
                        <a href="https://huggingface.co/collections/Jialuo21/science-t2i-67d3bfe43253da2bc7cfaf06" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>Pisa</span>
                        </a>
                    </div>
                </div>
                <div class="header-image">
                    <img src="static/img/teaser_img.png" alt="Teaser Image" class="teaser-image">
                </div>
            </div>
        </div>
    <d-article>
        <div class="byline">
            <div class="byline-container">
                <p>
                    <a href="https://jialuo-li.github.io/" class="author-link" target="_blank" style="margin-right: 15px;">Jialuo Li<sup>△</sup></a>
                    <a href="https://rese1f.github.io/" class="author-link" target="_blank" style="margin-right: 15px;">Wenhao Chai<sup>▲</sup></a>
                    <a href="https://zeyofu.github.io/" class="author-link" target="_blank">Xingyu Fu<sup>◆</sup></a>
                </p>
                <p>
                    <a href="https://xxuhaiyang.github.io/" class="author-link" target="_blank" style="margin-right: 15px;">Haiyang Xu<sup>◇</sup></a>
                    <a href="https://www.sainingxie.com/" class="author-link" target="_blank">Saining Xie<sup>△</sup></a>
                </p>
                <p></p>
                <p></p>
                <p style="text-align: center;">
                    <a class="affiliation-link" target="_blank" style="margin-right: 15px;"><sup>△</sup>New York University</a>
                    <a class="affiliation-link" target="_blank"><sup>▲</sup>University of Washington</a>
                </p>
                <p style="text-align: center;">
                    <a class="affiliation-link" target="_blank" style="margin-right: 15px;"><sup>◆</sup>University of Pennsylvania</a>
                    <a class="affiliation-link" target="_blank"><sup>◇</sup>University of California, San Diego</a>
                </p>

            </div>
        </div>
        
        <d-figure id="fig-teaser">
            <figure>
                <img data-zoomable="" draggable="false" src="static/img/teaser.png" alt="Science-T2I">
                <figcaption>
                    <strong>Fig 1:</strong> When presented with knowledge-implicit prompts, can LMMs and VLMs effectively distinguish between real and fake scientific images? Can generative models produce scientifically plausible images from such prompts? Does fine-tuning generative models with relevant data enhance their ability to generalize based on knowledge? To explore these questions, we establish a benchmark for evaluating LMMs and VLMs, construct a dataset to train a reward model which can then serve as a reliable tool for assessing generative models, and fine-tune generative models to investigate the generalization.
                </figcaption>
            </figure>
        </d-figure>

        <p class="text abstract">
            We present a novel approach to integrating scientific knowledge into generative models, enhancing their realism and consistency in image synthesis. First, we introduce <b>Science-T2I</b>, an expert-annotated adversarial dataset comprising adversarial 20k image pairs with 9k prompts, covering wide distinct scientific knowledge categories. Leveraging <b>Science-T2I</b>, We present <b>SciScore</b>, an end-to-end reward model that refines the assessment of generated images based on scientific knowledge, which is achieved by augmenting both the scientific comprehension and visual capabilities of pre-trained CLIP model. Additionally, based on <b>Science-T2I</b>, we propose a two-stage training framework, comprising a supervised fine-tuning phase and a masked online fine-tuning phase, to incorporate scientific knowledge into existing generative models. Through comprehensive experiments, we demonstrate the effectiveness of our framework in establishing new standards for evaluating the scientific realism of generated content. Specifically, <b>SciScore</b> attains performance comparable to human-level, demonstrating a 5% improvement similar to evaluations conducted by experienced human evaluators. Furthermore, by applying our proposed fine-tuning method to FLUX, we achieve a performance enhancement exceeding 50% based on <b>SciScore</b>.
        </p>
        <div class="icon-row">
            <a href="#pisabench" class="icon-link">
                <img src="static/img/icons/benchmark.svg" alt="Pisa Bench Logo" class="icon-jump">
                PisaBench<br> &nbsp;
            </a>
            <a href="#post-training" class="icon-link">
                <img src="static/img/icons/enhancement.svg" alt="Physics Post-Training Logo" class="icon-jump">
                Physics<br>Post-Training
            </a>
            <a href="#generalization" class="icon-link">
                <img src="static/img/icons/generalization.svg" alt="Generalization Analysis Logo" class="icon-jump">
                Generalization<br>Analysis
            </a>
        </div>
        <p class="click-hint2" style="width: 85%;">
            <img src="static/img/icon/click.gif" style="width: 1.5rem">
            <strong>Click to jump to each section.</strong>
        </p>
        <hr>

        <div id="science-t2i">
            <h1 class="text">Science-T2I: An Adversarial Dataset Spanning Scientific Disciplines</h1>
            <p class="text">
                <strong>Task overview.</strong> Science-T2I consists of 16 tasks spanning physics, chemistry and biology that require the model to infer or visualize concepts not explicitly stated in the prompts but rooted in underlying scientific principles.
            </p>
            
            <d-figure id="fig-data-stat">
              <figure>
                  <img data-zoomable="" draggable="false" src="static/img/data_stat.png">
                  <figcaption style="text-align: center; margin: 20px;">
                    <strong>Fig 2:</strong> Data statistics (left) of and wordcloud (right) of Science-T2I.
                  </figcaption>
              </figure>
            </d-figure>
  
            <p class="text">
                <strong>Task classification.</strong> Beyond a classification based on scientific disciplines, the tasks can be categorized into two distinct groups:
            </p>
            
            <p class="text">
                1. <strong>Subject-oriented task (ST)</strong> require scientific reasoning to discern how inherent differences between subjects lead to varying visual features under identical conditions.
            </p>

            <p class="text">
                2. <strong>Condition-oriented task (CT)</strong> focus on how a single condition affects various subjects. Scientific reasoning in these tasks centers on the applied condition, not the subject's individual properties.
            </p>

            <d-figure id="fig-classification">
              <figure>
                  <img data-zoomable="" draggable="false" src="static/img/classification.png">
                  <figcaption style="text-align: center; margin: 20px;">
                    <strong>Fig 3:</strong> Task classification of Science-T2I.
                  </figcaption>
              </figure>
            </d-figure>
  
            <p class="text">
                <strong>Prompt design.</strong> In Science-T2I, we categorize prompts into three types based on their use in scientific reasoning:
            </p>

            <p class="text">
                1. <strong>Implicit prompt (IP)</strong> refers to prompts that imply visual characteristics or phenomena requiring scientific interpretation. For example, "an unripe apple" suggests the apple's color is green, but this is not explicitly stated.
            </p>

            <p class="text">
                2. <strong>Explicit prompt (EP)</strong> reformulates the IP into a clear, descriptive statement that results in a scientifically accurate depiction. For instance, "a green apple" explicitly conveys the apple's immaturity.
            </p>

            <p class="text">
                3. <strong>Superficial prompt (SP)</strong> provides explicit but scientifically inaccurate descriptions, focusing on surface-level interpretations. For example, interpreting "an unripe apple" as "a red apple" is a superficial interpretation that lacks scientific accuracy.
            </p>
            
            <d-figure id="fig-data-pipeline">
              <figure>
                  <img data-zoomable="" draggable="false" src="static/img/dataset_pipeline.png">
                  <figcaption style="text-align: center; margin: 20px;">
                    <strong>Fig 4:</strong> Data curation pipeline of Science-T2I.
                  </figcaption>
              </figure>
            </d-figure>
  
            <p class="text"><strong>Data curation.</strong> We utilize GPT-4o to create templates and generate corresponding prompts during the data curation process. These outputs are then used to guide T2I models for image generation. Following this, human annotators review and filter the data, incorporating insights from an additional website knowledgebase to ensure the reliability and accuracy of the final dataset.
            </p>
            
            
            <h2 class="title is-3" style="text-align: center;">Science-T2I Examples</h2>
            <div id="results-carousel" class="carousel results-carousel text" style="height: 500px;">  
            
                <div class="content has-text-centered">
                <img src="static/img/cases/absorption.png" style="width: 90%; margin-left: 60px;"/>
                <br>
                </div>
                <div class="content has-text-centered">
                <img src="static/img/cases/buoyancy.png" style="width: 90%; margin-left: 60px;"/>
                <br>
                </div>
                <div class="content has-text-centered">
                <img src="static/img/cases/dark.png" style="width: 90%; margin-left: 60px;"/>
                <br>
                </div>
                <div class="content has-text-centered">
                <img src="static/img/cases/diffusion.png" style="width: 90%; margin-left: 60px;"/>
                <br>
                </div>
                <div class="content has-text-centered">
                <img src="static/img/cases/electricity.png" style="width: 90%; margin-left: 60px;"/>
                <br>
                </div>
                <div class="content has-text-centered">
                <img src="static/img/cases/evaporation.png" style="width: 90%; margin-left: 60px;"/>
                <br>
                </div>
                <div class="content has-text-centered">
                <img src="static/img/cases/flame.png" style="width: 90%; margin-left: 60px;"/>
                <br>
                </div>
                <div class="content has-text-centered">
                <img src="static/img/cases/gravity.png" style="width: 90%; margin-left: 60px;"/>
                <br>
                </div>
                <div class="content has-text-centered">
                <img src="static/img/cases/immisciblity.png" style="width: 90%; margin-left: 60px;"/>
                <br>
                </div>
                <div class="content has-text-centered">
                <img src="static/img/cases/liquidation.png" style="width: 90%; margin-left: 60px;"/>
                <br>
                </div>
                <div class="content has-text-centered">
                <img src="static/img/cases/melting.png" style="width: 90%; margin-left: 60px;"/>
                <br>
                </div>
                <div class="content has-text-centered">
                <img src="static/img/cases/ripeness.png" style="width: 90%; margin-left: 60px;"/>
                <br>
                </div>
                <div class="content has-text-centered">
                <img src="static/img/cases/rust.png" style="width: 90%; margin-left: 60px;"/>
                <br>
                </div>
                <div class="content has-text-centered">
                <img src="static/img/cases/solidification.png" style="width: 90%; margin-left: 60px;"/>
                <br>
                </div>
                <div class="content has-text-centered">
                <img src="static/img/cases/tree.png" style="width: 90%; margin-left: 60px;"/>
                <br>
                </div>
                <div class="content has-text-centered">
                <img src="static/img/cases/water.png" style="width: 90%; margin-left: 60px;"/>
                <br>
                </div>
            </div>
            <br>
            <div class="collapsible-section">
                <button class="button is-fullwidth toggle-section" aria-controls="data_table" style="background-color: white; width: 150%; margin-left: -20%;">
                    <span>View trainset</span>
                    <span class="icon is-small">
                        <i class="fas fa-angle-down" aria-hidden="true"></i>
                    </span>
                </button>
                <div id="data_table" class="collapse-content text" style="display: none; width: 80%;">
                  <iframe
                  src="https://huggingface.co/datasets/Jialuo21/Science-T2I-Trainset/embed/viewer/default/train"
                  frameborder="0"
                  style="width: 187%; height: 600px; border: none;"
                  ></iframe>
                </div>
                <script>
                    document.addEventListener('DOMContentLoaded', function() {
                        const button = document.querySelector('[aria-controls="data_table"]');
                        if (button) {
                            button.addEventListener('click', function() {
                                const content = document.getElementById('data_table');
                                if (content.style.display === 'none') {
                                    content.style.display = 'block';
                                } else {
                                    content.style.display = 'none';
                                }
                            });
                            button.click(); // 默认展开
                        }
                    });
                </script>
            </div>
        </div>
        <div id="generalization">
            <h1 class="text">Generalization Analysis</h1>
            <p class="text">
                <p class="text">
                Having introduced our post-training methodology, we probe further into the model's understanding of the interaction between gravity and perspective, the two laws that determine the dynamics of our videos. We first test if the learned physical behavior of our model can generalize to dropping heights and depths beyond its training distribution. Then, we study the ability of the model to learn the probability distribution induced by the uncertainty of perspective.
                </p>
            </p> 
            <p class="text">
                <p class="text">
                    <strong>Generalization to Unseen Depths and Heights. </strong>
                    Depth and height are the main factors that affect the dynamics of a falling object in our videos. We can see this by combining the laws of gravity with perspective under our camera assumptions to model the object's image \(y\) coordinate as a function of time: 
                    \[
                    y(t) = \frac{f}{Z} (Y_0 - \frac{1}{2} g t^2)
                    \]
                </p>
            </p>
            <p class="text">
                <p class="text">
                    We create a simulated test set in which a single object is dropped from varying depths and heights, using objects and backgrounds unseen during training. We create an in-distribution (ID) and out-of-distribution (OOD) test set respectively.
                </p>
            </p> 
            <div id="tab:depth_height_tab" style="display: flex; flex-direction: column; align-items: center;">
                <div class="table-container">
                    <table class="data-table">
                        <thead>
                        <tr>
                            <th colspan="1" class="tb-hdr">Setting</th>
                            <th colspan="1" class="tb-hdr">L2 (\(\downarrow\))</th>
                            <th colspan="1" class="tb-hdr">Chamfer Distance (\(\downarrow\))</th>
                            <th colspan="1" class="tb-hdr">IoU (\(\uparrow\))</th>
                            <th colspan="1" class="tb-hdr">Time Error (\(\downarrow\))</th>
                        </tr>
                        </thead>
                        <tbody>
                        <tr>
                            <td class="section-border">ID</td>
                            <td class="highlight">0.036</td>
                            <td class="highlight">0.088</td>
                            <td class="highlight">0.155</td>
                            <td class="highlight">0.049</td>
                        </tr>
                        <tr>
                            <tr>
                                <td class="section-border">OOD</td>
                                <td>0.044</td>
                                <td>0.143</td>
                                <td>0.049</td>
                                <td>0.187</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <figcaption>
                    <strong>Table 2: Results of our metrics on in-distribution (ID) and out-of-distribution (OOD) depth-height combinations. </strong> Depth values range from 1-5m (ID range \([1,3]\)) and height values range from 0.5-2.5 (ID range \([0.5,1.5]\)).
                </figcaption>
            </div>
            <p class="text">
                <p class="text">
                    As shown in <a href="#tab:depth_height_tab">Table 2</a>, our analysis shows that performance degrades for out-of-distribution scenarios. Since depth and height are the main physical quantities that affect falling dynamics, this finding indicates that our model may struggle to learn a fully generalizable law that accounts for the interaction of perspective and gravity.
                </p>
            </p>
            <p class="text">
                <p class="text">
                    <strong>Distributional Analysis.</strong>
                    The evolution of a physical system is not uniquely determined by a single initial image, since the lossy uncertainty of perspective induces a distribution of possible outcomes as shown in <a href="fig:ambiguity">Figure 5</a>. An ideal video world model should (1) output videos that are faithful to the evolution of some plausible world state and (2) provide accurate coverage across the entire distribution of the world that is possible from its conditioning signal. In this section, we examine these two facets by studying \(p(t|y)\): the distribution of dropping times possible from an object at coordinate y in the image plane. To do this, we create a simulated dataset that has a much wider distribution \(p(t|y)\) than our PSFT dataset.
                </p>
            </p>
            <d-figure id="fig:ambiguity">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/ambiguity.png" alt="ambiguity figure">
                    <figcaption>
                        <strong>Figure 5. </strong>Demonstration of ambiguity in 2D perspective projections. Each of the three clouds appears the exact same in the camera's image. The right side shows how we perform a scale and translation augmentation to generate deliberately ambiguous data.
                    </figcaption>
                </figure>
            </d-figure>
            <p class="text">
                <p class="text">
                    <strong>Testing (1): 3D faithfulness of trajectories.</strong>
                    After training our model on this new dataset, we test whether its trajectories are consistent with a valid 3D world state. We first obtain an estimated dropping time from generated videos using SAM2 masks. Using knowledge of the camera position, focal length, sensor width, and \(y\), we can obtain an implied depth and height of the trajectory. We can then back-project the video trajectory to 3D and analyze whether they constitute physically accurate trajectories. As show in in <a href="#fig:testing1">Figure 6</a>, we find that our model's lifted trajectories consistently align with the 3D trajectory at the height and depth implied by its dropping time, giving evidence that the model's visual outputs are faithful to some plausible real-world state.
                </p>
            </p>
            <d-figure id="fig:testing1">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/testing1.png" alt="testing1 figure">
                    <figcaption>
                        <strong>Figure 6: Examples of model trajectories lifted to 3D. </strong>The blue line represents the height of the camera ray passing through the bottom of the dropping object as a function of depth. The set of possible dropping trajectories at a given depth are depicted in gray. The lifted trajectory of the model is depicted in green.
                    </figcaption>
                </figure>
            </d-figure>

            <p class="text">
                <p class="text">
                    <strong>Testing (2): distributional alignment.</strong>
                    Going beyond the level of individual trajectories, we study the model's learned conditional distribution \(p(t|y)\). We create 50 different initial images with differing values of y, generate 128 different videos from each, and estimate the dropping time in each video. Using the laws of gravity, the laws of perspective, and the assumption of uniform depth sampling in our dataset, we can analytically derive the probability \(p(t|y)\) as: 
                    \[
                    \begin{equation}
                    p(t | y) =
                    \begin{cases}
                        \frac{gt}{(Z_{\max} - Z_{\min})\beta}, & t_{\min} \leq t \leq t_{\max} \\
                        0, & \text{otherwise}
                    \end{cases}
                    \end{equation}
                    \]
                </p>
            </p>
            <p class="text">
                <p class="text">
                    We then measure goodness-of-fit for each of the 50 experiments using the Kolmogorov-Smirnov (KS). The null hypothesis of our KS test is that the two distributions being compared are equal, and we consider p-values less than 0.05 as evidence of misalignment. Since our measured times have limited precision and can only take 32 distinct values—due to estimating the contact frame—we approximate the ground truth \(p(t|y)\) using a Monte Carlo method. We sample 1000 values from the ground truth distribution and then quantized them into 32 bins corresponding to their frame, which we use as ground truth observations in the KS test. We find that in all 50/50 cases, the p-value from the test is less than 0.05, which provides evidence that the model does not learn the correct distribution of dropping times. We visualize the misalignment between the empirical cdf of the model's in <a href="fig:testing2">Figure 7</a>.
                </p>
            </p>
            <d-figure id="fig:testing1">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/testing2.png" alt="testing1 figure">
                    <figcaption>
                        <strong>Figure 7: Visualizing \(p(t|y)\) misalignment for different images.</strong>Green shows the ground-truth CDF, orange is the 32-frame quantized version, and blue is the empirical CDF of 128 different samples of dropping times from the model.
                    </figcaption>
                </figure>
            </d-figure>
        </div>
        <div id="Conclusion">
            <h1 class="text">Conclusion</h1>
            <p class="text">
                <p class="text">
                    This work studies post-training as an avenue for adapting adapting pre-trained video generator into world models. We introduce a post-training strategy that is highly effective in aligning our model. Our work raises interesting insights into the learned distributions of generative models. Qualitatively, large scale image or video generative models appear to excel at generating likely samples from the data distribution, but this alone does not imply that they match the data distribution well in its entirety. As long as a model is able to generate likely samples, global distributional misalignment is not necessarily a problem for content creation. However, this problem becomes critical for world models, where alignment across the entire distribution is necessary for faithful world simulation. The insights revealed by our study, made possible by our constrained and tractable setting, indicate that although post-training improves per-sample accuracy, general distributional alignment remains unsolved.
                </p>
            </p>
        </div>

        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{li2025pisa,<br>
                &nbsp;&nbsp;title={PISA Experiments: Exploring Physics Post-Training for Video Diffusion Models by Watching Stuff Drop},<br>
                &nbsp;&nbsp;author={Li, Chenyu and Michel, Oscar and Pan, Xichen and Liu, Sainan and Roberts, Mike and Xie, Saining},<br>
                &nbsp;&nbsp;journal={arXiv preprint arXiv:2503.09595},<br>
                &nbsp;&nbsp;year={2025}<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>
        
        <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>
        <script src="./static/js/nav-bar.js"></script>
        
        <script type="text/javascript">
        </script>
    </body>
</html>
